spark.master                            spark://spark-master:7077
spark.driver.extraLibraryPath           /opt/hadoop/lib/native
#spark.driver.extraClassPath             /opt/spark/jars/hadoop-aws-2.8.2.jar:/opt/spark/jars/aws-java-sdk-1.11.712.jar
#spark.hadoop.fs.s3a.impl                org.apache.hadoop.fs.s3a.S3AFileSystem
#spark.fs.s3a.connection.ssl.enabled     false
#spark.executor.extraJavaOptions         -Dcom.amazonaws.sdk.disableCertChecking=1
spark.app.id                            KubernetesSpark


#-------------------------------------------------------
# This setting is being added since kubernetes doesn't
# add pod name(hostname) into the kubernetes's DNS, but
# spark workder's using podname as driver's hostname,
# since it is not in DNS, spark workers can not resolve
# master's podname IP, so spark workers won't be able to
# communicate back to driver, which cauese the executor job
# submission failure.
# 
# Because of this setting, we can only set console driver
# on master node
#
# If you deploy spark in docker environment, comment
# out this setting, and rebuild docker image.
# 
# There are two prebuilt docker images: 
# 1. stanleywxc/spark:kube2.4.5 for kuberntes deployment
# 2. stanleywxc/spark:docker2.4.5 for docker-compose deployment.
#
#--------------------------------------------------------

# Use the following setting in kubernetes
spark.driver.host spark-master-headless

# Use the following setting in docker
#spark.driver.host spark-master